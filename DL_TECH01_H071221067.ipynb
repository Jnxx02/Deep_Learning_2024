{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "y_true = [1, 0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "5glyBDmEV_KA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnQSfw77Ubcb",
        "outputId": "739ed9c4-03b9-42c6-bb92-c1c2edf927b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP: 3, TN: 2, FP: 0, FN: 1\n"
          ]
        }
      ],
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "  TP = sum((y_true[i] == 1 and y_pred[i] == 1) for i in range(len(y_true)))\n",
        "  TN = sum((y_true[i] == 0 and y_pred[i] == 0) for i in range(len(y_true)))\n",
        "  FP = sum((y_true[i] == 0 and y_pred[i] == 1) for i in range(len(y_true)))\n",
        "  FN = sum((y_true[i] == 1 and y_pred[i] == 0) for i in range(len(y_true)))\n",
        "\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "TP, TN, FP, FN = confusion_matrix(y_true, y_pred)\n",
        "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(TP, TN, FP, FN):\n",
        "  return (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "  accurac_value = accuracy(TP, TN, P, FN)\n",
        "  print(f\"Accuracy: {accuracy_value}\")"
      ],
      "metadata": {
        "id": "vpM9oIOxV-bt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(TP, FP):\n",
        "  if TP + FP == 0:\n",
        "    return 0\n",
        "  return TP / (TP + FP)\n",
        "\n",
        "precision_value = precision(TP, FP)\n",
        "print(f\"Precision: {precision_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaOmqjCwWPE4",
        "outputId": "42109d5f-a356-4f55-fa05-79e14555d10c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(TP, FN):\n",
        "  if TP + FN == 0:\n",
        "    return 0\n",
        "  return TP / (TP + FN)\n",
        "\n",
        "recall_value = recall(TP, FN)\n",
        "print(f\"Recall: {recall_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW57YotyWa1c",
        "outputId": "0ed22d39-77bb-4997-e000-27e24e989057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score(precision, recall):\n",
        "  if precision + recall == 0:\n",
        "    return 0\n",
        "  return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "f1_value = f1_score(precision_value, recall_value)\n",
        "print(f\"F1 Score: {f1_value:.2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY5bV9MbWnd3",
        "outputId": "b4da0ac2-63e5-4519-d0e1-ffd7866cd024"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "y_true = [1, 1, 0, 0, 1]\n",
        "y_pred = [1, 0, 0, 1, 1]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6oWU6vnWlRq",
        "outputId": "40baa27c-4321-4c54-cd57-c4796391c3bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[1 1]\n",
            " [1 2]]\n",
            "Accuracy: 0.60\n",
            "Precision: 0.67\n",
            "Recall: 0.67\n",
            "F1 Score: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. What is a confusion matrix used for in machine learning?**\n",
        "Answer: Confusion matrix is a tool on machine learning that used for evaluate the performance of a classification model by comparing the predicted values with actual values for a dataset.\n",
        "\n",
        "# **2. Given the confusion matrix below, what is the True Positive (TP) count?**\n",
        "## Actual Positive/Predicted Positive: 3\n",
        "## Actual Positive/Predicted Negative: 1\n",
        "## Actual Negative/Predicted Positive: 2\n",
        "## Actual Positive/Predicted Negative: 4\n",
        "Answer: True Positive: 3 (Actual Positive/ Predicted Positive)\n",
        "\n",
        "# **3. How is precision calculated from a confusion matrix?**\n",
        "Answer: Precision = TP / (TP + FP)\n",
        "\n",
        "# **4. Using the following confusion matrix, calculate the accuracy.**\n",
        "## Actual Positive/Predicted Positive: 5\n",
        "## Actual Positive/Predicted Negative: 3\n",
        "## Actual Negative/Predicted Positive: 2\n",
        "## Actual Positive/Predicted Negative: 4\n",
        "Answer: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "                = (5 + 4) / (5 + 4 + 2 + 3) = 9 / 14 = 0.64\n",
        "\n",
        "\n",
        "# **5. What is the F1 score if the Precision is 0.8 and the recall is 0.6?**\n",
        "Answer: F1 score = 2 * (precision * recall) / (precision + recall)\n",
        "                = 2 * (0.8 * 0.6) / (0.8 + 0.6)\n",
        "                = 2 * (0.48) / (1.4)\n",
        "                = 0.96 / 1.4\n",
        "                = 0.685"
      ],
      "metadata": {
        "id": "qImp6TFsXy5G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rB_7F06OXp6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}